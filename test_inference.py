#!/usr/bin/env python3
"""
DeepSeek-OCR æ¨ç†æµ‹è¯•è„šæœ¬
è¯¦ç»†æµ‹è¯•æµç¨‹å’Œè¾“å‡ºç»“æœè¯´æ˜
"""

import os
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def test_inference(image_path, output_dir, prompt_type="markdown"):
    """
    æµ‹è¯•DeepSeek-OCRæ¨¡å‹æ¨ç†
    
    Args:
        image_path: è¾“å…¥å›¾ç‰‡è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        prompt_type: æç¤ºè¯ç±»å‹ (markdown/free/detection)
    """
    
    print("ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹...")
    
    # æ­¥éª¤1: åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model_name = "deepseek-ai/DeepSeek-OCR"
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name, 
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        _attn_implementation='flash_attention_2'
    )
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    
    # æ­¥éª¤2: è®¾ç½®æç¤ºè¯
    prompts = {
        "markdown": "<image>\n<|grounding|>Convert the document to markdown.",
        "free": "<image>\n<|grounding|>Extract all text and images from this document.",
        "detection": "<image>\n<|grounding|>Detect all text regions and images in this document."
    }
    
    prompt = prompts.get(prompt_type, prompts["markdown"])
    print(f"ğŸ“ ä½¿ç”¨æç¤ºè¯: {prompt}")
    
    # æ­¥éª¤3: æ‰§è¡Œæ¨ç†
    print(f"ğŸ–¼ï¸  å¤„ç†å›¾ç‰‡: {image_path}")
    
    result = model.infer(
        prompt=prompt,
        image_file=image_path,
        output_path=output_dir,
        base_size=640,      # åŸºç¡€å°ºå¯¸
        image_size=1280,    # å›¾åƒå°ºå¯¸
        crop_mode="dynamic", # åŠ¨æ€è£å‰ªæ¨¡å¼
        save_results=True,   # ä¿å­˜ç»“æœ
        test_compress=True   # æµ‹è¯•å‹ç¼©
    )
    
    print("âœ… æ¨ç†å®Œæˆï¼")
    print(f"ğŸ“‚ ç»“æœä¿å­˜åœ¨: {output_dir}")
    
    return result

def analyze_output(output_dir):
    """
    åˆ†æè¾“å‡ºç»“æœ
    """
    print("\n" + "="*50)
    print("ğŸ“Š è¾“å‡ºç»“æœåˆ†æ")
    print("="*50)
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    result_mmd = os.path.join(output_dir, "result.mmd")
    result_boxes = os.path.join(output_dir, "result_with_boxes.jpg")
    images_dir = os.path.join(output_dir, "images")
    
    print(f"ğŸ“„ Markdownç»“æœ: {result_mmd}")
    if os.path.exists(result_mmd):
        with open(result_mmd, 'r', encoding='utf-8') as f:
            content = f.read()
            print(f"   - æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
            print(f"   - åŒ…å«å›¾åƒå¼•ç”¨: {content.count('![](')} ä¸ª")
    
    print(f"ğŸ–¼ï¸  æ ‡æ³¨å›¾åƒ: {result_boxes}")
    if os.path.exists(result_boxes):
        size = os.path.getsize(result_boxes)
        print(f"   - æ–‡ä»¶å¤§å°: {size/1024:.1f} KB")
    
    print(f"ğŸ“ æå–å›¾åƒç›®å½•: {images_dir}")
    if os.path.exists(images_dir):
        images = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]
        print(f"   - æå–å›¾åƒæ•°é‡: {len(images)} ä¸ª")
        for img in sorted(images):
            img_path = os.path.join(images_dir, img)
            size = os.path.getsize(img_path)
            print(f"   - {img}: {size/1024:.1f} KB")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="DeepSeek-OCR æ¨ç†æµ‹è¯•")
    parser.add_argument("--image", required=True, help="è¾“å…¥å›¾ç‰‡è·¯å¾„")
    parser.add_argument("--output", required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--prompt", choices=["markdown", "free", "detection"], 
                       default="markdown", help="æç¤ºè¯ç±»å‹")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output, exist_ok=True)
    
    # æ‰§è¡Œæµ‹è¯•
    result = test_inference(args.image, args.output, args.prompt)
    
    # åˆ†æç»“æœ
    analyze_output(args.output)
